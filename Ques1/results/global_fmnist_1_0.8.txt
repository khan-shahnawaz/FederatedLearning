Arguments:
	              alpha : 0.0
	         batch_size : 32
	           boosting : 0
	  clients_per_round : 10
	          comm_freq : 0.1
	data_partition_seed : 0
	            dataset : fmnist
	       decay_factor : 1.0
	        dynamic_lam : 0
	         eval_every : 10
	            fedmgda : 0
	        fedmgda_eps : 0.0
	     finetune_iters : 40
	         global_reg : -1.0
	  gradient_clipping : 0
	             k_loss : 0
	             k_norm : 0
	               krum : 0
	                lam : 100.0
	       lambda_l2sgd : 0
	      learning_rate : 0.1
	        local_iters : 2
	             median : 0
	              mkrum : 0
	              model : cnn
	       model_params : (10,)
	      num_corrupted : 400
	         num_epochs : 1
	         num_rounds : 1000
	          optimizer : ditto
	                  q : 0.0
	     random_updates : 0
	           sampling : 2
	               seed : 0
Using global-regularized multi-task learning to Train
Parsing Inputs...

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/6.55m flops)
  dense/kernel/Initializer/random_uniform (3.21m/6.42m flops)
    dense/kernel/Initializer/random_uniform/mul (3.21m/3.21m flops)
    dense/kernel/Initializer/random_uniform/sub (1/1 flops)
  conv2d_1/kernel/Initializer/random_uniform (51.20k/102.40k flops)
    conv2d_1/kernel/Initializer/random_uniform/mul (51.20k/51.20k flops)
    conv2d_1/kernel/Initializer/random_uniform/sub (1/1 flops)
  dense_1/kernel/Initializer/random_uniform (10.24k/20.48k flops)
    dense_1/kernel/Initializer/random_uniform/mul (10.24k/10.24k flops)
    dense_1/kernel/Initializer/random_uniform/sub (1/1 flops)
  conv2d/kernel/Initializer/random_uniform (800/1.60k flops)
    conv2d/kernel/Initializer/random_uniform/mul (800/800 flops)
    conv2d/kernel/Initializer/random_uniform/sub (1/1 flops)
  gradients_1/Sum_1_grad/Maximum (2/2 flops)
  gradients_1/Sum_grad/Maximum (2/2 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)
  sparse_softmax_cross_entropy_loss/div (1/1 flops)
  sparse_softmax_cross_entropy_loss/Greater (1/1 flops)
  sparse_softmax_cross_entropy_loss/Equal (1/1 flops)
  gradients_1/Sum_grad/add (1/1 flops)
  gradients_1/Sum_1_grad/add (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/mul (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/RealDiv_2 (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/RealDiv_1 (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/RealDiv (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/Neg (1/1 flops)

======================End of Report==========================
500 Clients in Total
---10 workers per communication round---
[394 276 255 318 369 457 458  75  62 113 266 365 300 268 250 389 267   9
 378 112 433 436 254 440  57 232 396  19 181  29 398  94 228 265 249  68
 244 163  99 464 468 216 156 449 135  82 284 296 118 331 353 357 351 328
  14  91 311  35 476 167 136 363 119 213 346 205 307 207  43 362 230 337
 417  64 274 122  54  38 321 489 390 409 152 234  98  20 483 187 339 243
 428 208 303 221 299 427  55 329 271 402 320 170 165 178  17 404 106 438
   5 200 214 183 235  73 292 416 301 211 453 494 488 345 264  63 354 322
 225 157 277 309  23  22 335  16  44 371 499 233 151 355 180 154 162 442
 444 217  69 141 475 189  87 490  95 204 340  27 281 297 374 308 148  21
 386 176 344 100 446  65 231 137 238 131  67 290 429  49 443 251  92 146
  34 498 134 253 376 120 388 242 349 432 392 456   8  47 272 413 226  77
 410   7 358  15 342 405 202 343 160   3  60  31 247  74 177 169 481 192
 372 223  33  13 139 222  28 287 144  51 285  59 198 495 260 257 179 470
 425  90 164 484 347 282  78 304 469  56 132 129 193 195 280 452 191 182
 172 155 348  81 479 270 418 194  52 323 364  30 496 375 384 145 450 324
 367 305 278  12 448 431 121 313 497  41 377 333 123  24 338 269  37 229
   6  88 101 210 401 206 393 124 115 294 197 199 220  48 408  11 491  10
 312 461 352 263 317 420 107 316 224 138 466  86 117 273  50 382 286 435
  32 149 447 102  39 319 239 310 412 168 245   4  83 293  93 291 314  66
 341  71 385 315   0  89  70 246 395 174 455 133 158 262 441 110  80 283
 159 430 248 424 188 190 474 147 184 185 366 415 108 173 472 252 486 196
  26 381 383 289  36 142 153   2 462 336 485 203 368 241 209 130 237 298
 105  46 109 473]
At round 10 training accu: 0.0999983673735939, loss: nan
At round 10 test accu: 0.08068242402701262
At round 10 malicious test accu: 0.07601408930803318
At round 10 benign test accu: 0.09743171626579698
variance of the performance:  0.017441773841185473
At round 20 training accu: 0.10634928409332092, loss: nan
At round 20 test accu: 0.09383330371423494
At round 20 malicious test accu: 0.08464947165094876
At round 20 benign test accu: 0.1267835303709743
variance of the performance:  0.023442946128407964
At round 30 training accu: 0.10234934939837717, loss: nan
At round 30 test accu: 0.08894615247911854
At round 30 malicious test accu: 0.083740484035905
At round 30 benign test accu: 0.10762331838565023
variance of the performance:  0.020305888776634286
At round 40 training accu: 0.10798191049942042, loss: nan
At round 40 test accu: 0.09054558379242936
At round 40 malicious test accu: 0.07953641631632768
At round 40 benign test accu: 0.13004484304932734
variance of the performance:  0.023625949123376587
At round 50 training accu: 0.10572888605900312, loss: nan
At round 50 test accu: 0.09383330371423494
At round 50 malicious test accu: 0.08726281104419953
At round 50 benign test accu: 0.11740725642070933
variance of the performance:  0.02317838130437603
At round 60 training accu: 0.10770436401038351, loss: nan
At round 60 test accu: 0.09916474142527101
At round 60 malicious test accu: 0.09203499602317919
At round 60 benign test accu: 0.12474520994700367
variance of the performance:  0.024532429204495294
At round 70 training accu: 0.11047982890075264, loss: nan
At round 70 test accu: 0.10582903856406611
At round 70 malicious test accu: 0.09657993409839791
At round 70 benign test accu: 0.13901345291479822
variance of the performance:  0.025848393130894053
At round 80 training accu: 0.10827578325251833, loss: nan
At round 80 test accu: 0.10289674782299627
At round 80 malicious test accu: 0.09567094648335417
At round 80 benign test accu: 0.12882185079494496
variance of the performance:  0.025900438108175425
At round 90 training accu: 0.10871659238216519, loss: nan
At round 90 test accu: 0.10440732184112315
At round 90 malicious test accu: 0.09578456993523463
At round 90 benign test accu: 0.13534447615165104
variance of the performance:  0.026681889560939465
At round 100 training accu: 0.10791660544317644, loss: nan
At round 100 test accu: 0.10254131864226053
At round 100 malicious test accu: 0.09464833541642995
At round 100 benign test accu: 0.1308601712189156
variance of the performance:  0.026166961656701883
At round 110 training accu: 0.10910842271962906, loss: nan
At round 110 test accu: 0.1027190332326284
At round 110 malicious test accu: 0.09351210089762527
At round 110 benign test accu: 0.13575214023644516
variance of the performance:  0.028383085000827188
At round 120 training accu: 0.10850435094937226, loss: nan
At round 120 test accu: 0.10031988626266217
At round 120 malicious test accu: 0.09214861947505965
At round 120 benign test accu: 0.12963717896453322
variance of the performance:  0.02805395059692938
