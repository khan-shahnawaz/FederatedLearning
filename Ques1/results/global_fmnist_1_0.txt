Arguments:
	              alpha : 0.0
	         batch_size : 32
	           boosting : 0
	  clients_per_round : 10
	          comm_freq : 0.1
	data_partition_seed : 0
	            dataset : fmnist
	       decay_factor : 1.0
	        dynamic_lam : 0
	         eval_every : 10
	            fedmgda : 0
	        fedmgda_eps : 0.0
	     finetune_iters : 40
	         global_reg : -1.0
	  gradient_clipping : 0
	             k_loss : 0
	             k_norm : 0
	               krum : 0
	                lam : 100.0
	       lambda_l2sgd : 0
	      learning_rate : 0.1
	        local_iters : 2
	             median : 0
	              mkrum : 0
	              model : cnn
	       model_params : (10,)
	      num_corrupted : 0
	         num_epochs : 1
	         num_rounds : 1000
	          optimizer : ditto
	                  q : 0.0
	     random_updates : 0
	           sampling : 2
	               seed : 0
Using global-regularized multi-task learning to Train
Parsing Inputs...

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/6.55m flops)
  dense/kernel/Initializer/random_uniform (3.21m/6.42m flops)
    dense/kernel/Initializer/random_uniform/mul (3.21m/3.21m flops)
    dense/kernel/Initializer/random_uniform/sub (1/1 flops)
  conv2d_1/kernel/Initializer/random_uniform (51.20k/102.40k flops)
    conv2d_1/kernel/Initializer/random_uniform/mul (51.20k/51.20k flops)
    conv2d_1/kernel/Initializer/random_uniform/sub (1/1 flops)
  dense_1/kernel/Initializer/random_uniform (10.24k/20.48k flops)
    dense_1/kernel/Initializer/random_uniform/mul (10.24k/10.24k flops)
    dense_1/kernel/Initializer/random_uniform/sub (1/1 flops)
  conv2d/kernel/Initializer/random_uniform (800/1.60k flops)
    conv2d/kernel/Initializer/random_uniform/mul (800/800 flops)
    conv2d/kernel/Initializer/random_uniform/sub (1/1 flops)
  gradients_1/Sum_1_grad/Maximum (2/2 flops)
  gradients_1/Sum_grad/Maximum (2/2 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)
  sparse_softmax_cross_entropy_loss/div (1/1 flops)
  sparse_softmax_cross_entropy_loss/Greater (1/1 flops)
  sparse_softmax_cross_entropy_loss/Equal (1/1 flops)
  gradients_1/Sum_grad/add (1/1 flops)
  gradients_1/Sum_1_grad/add (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/mul (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/RealDiv_2 (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/RealDiv_1 (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/RealDiv (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/Neg (1/1 flops)

======================End of Report==========================
500 Clients in Total
---10 workers per communication round---
[]
At round 10 training accu: 0.09178625655091345, loss: nan
At round 10 test accu: 0.0909010129731651
At round 10 malicious test accu: nan
At round 10 benign test accu: 0.0909010129731651
variance of the performance:  0.011913255281983667
At round 20 training accu: 0.11537770811905111, loss: nan
At round 20 test accu: 0.11391505242580416
At round 20 malicious test accu: nan
At round 20 benign test accu: 0.11391505242580416
variance of the performance:  0.014996902826744887
At round 30 training accu: 0.11753277497510245, loss: nan
At round 30 test accu: 0.11693620046205794
At round 30 malicious test accu: nan
At round 30 benign test accu: 0.11693620046205794
variance of the performance:  0.015511309822110513
At round 40 training accu: 0.1257448857977829, loss: nan
At round 40 test accu: 0.12573307268526746
At round 40 malicious test accu: nan
At round 40 benign test accu: 0.12573307268526746
variance of the performance:  0.017380604468230705
At round 50 training accu: 0.1362589998530636, loss: nan
At round 50 test accu: 0.13639594810733963
At round 50 malicious test accu: nan
At round 50 benign test accu: 0.13639594810733963
variance of the performance:  0.01829573603901404
At round 60 training accu: 0.14370377626487732, loss: nan
At round 60 test accu: 0.14385996090279013
At round 60 malicious test accu: nan
At round 60 benign test accu: 0.14385996090279013
variance of the performance:  0.019378007191594666
At round 70 training accu: 0.15003836672054333, loss: nan
At round 70 test accu: 0.14928025590901012
At round 70 malicious test accu: nan
At round 70 benign test accu: 0.14928025590901012
variance of the performance:  0.02121488097881804
At round 80 training accu: 0.1434425560399014, loss: nan
At round 80 test accu: 0.1432379598365026
At round 80 malicious test accu: nan
At round 80 benign test accu: 0.1432379598365026
variance of the performance:  0.020789060796554966
At round 90 training accu: 0.1427242004212176, loss: nan
At round 90 test accu: 0.14261595877021505
At round 90 malicious test accu: nan
At round 90 benign test accu: 0.14261595877021505
variance of the performance:  0.021221627374004324
At round 100 training accu: 0.14001404058709246, loss: nan
At round 100 test accu: 0.13986138261951306
At round 100 malicious test accu: nan
At round 100 benign test accu: 0.13986138261951306
variance of the performance:  0.021269952864929862
At round 110 training accu: 0.1428221580055836, loss: nan
At round 110 test accu: 0.14234938688466323
At round 110 malicious test accu: nan
At round 110 benign test accu: 0.14234938688466323
variance of the performance:  0.02343137300939086
At round 120 training accu: 0.14107524775105712, loss: nan
At round 120 test accu: 0.14154967122800782
At round 120 malicious test accu: nan
At round 120 benign test accu: 0.14154967122800782
variance of the performance:  0.024243815683298628
At round 130 training accu: 0.14254461151654668, loss: nan
At round 130 test accu: 0.14350453172205438
At round 130 malicious test accu: nan
At round 130 benign test accu: 0.14350453172205438
variance of the performance:  0.02479514142757688
At round 140 training accu: 0.13906711727155474, loss: nan
At round 140 test accu: 0.1404833836858006
At round 140 malicious test accu: nan
At round 140 benign test accu: 0.1404833836858006
variance of the performance:  0.02539845037861006
At round 150 training accu: 0.1398507779464825, loss: nan
At round 150 test accu: 0.14181624311355961
At round 150 malicious test accu: nan
At round 150 benign test accu: 0.14181624311355961
variance of the performance:  0.02612242626620918
At round 160 training accu: 0.13998138805897048, loss: nan
At round 160 test accu: 0.14234938688466323
At round 160 malicious test accu: nan
At round 160 benign test accu: 0.14234938688466323
variance of the performance:  0.026427245520870356
At round 170 training accu: 0.1366508301905275, loss: nan
At round 170 test accu: 0.13879509507730584
At round 170 malicious test accu: nan
At round 170 benign test accu: 0.13879509507730584
variance of the performance:  0.025242180102579122
At round 180 training accu: 0.1305121549035934, loss: nan
At round 180 test accu: 0.13284165629998224
At round 180 malicious test accu: nan
At round 180 benign test accu: 0.13284165629998224
variance of the performance:  0.024124975369282552
At round 190 training accu: 0.12946727400368974, loss: nan
At round 190 test accu: 0.13195308334814287
At round 190 malicious test accu: nan
At round 190 benign test accu: 0.13195308334814287
variance of the performance:  0.024906681839777463
At round 200 training accu: 0.12972849422866564, loss: nan
At round 200 test accu: 0.13159765416740715
At round 200 malicious test accu: nan
At round 200 benign test accu: 0.13159765416740715
variance of the performance:  0.02426998156015338
At round 210 training accu: 0.12974482049272665, loss: nan
At round 210 test accu: 0.13133108228185533
At round 210 malicious test accu: nan
At round 210 benign test accu: 0.13133108228185533
variance of the performance:  0.024603397721713632
At round 220 training accu: 0.12804688903038317, loss: nan
At round 220 test accu: 0.1294650790829927
At round 220 malicious test accu: nan
At round 220 benign test accu: 0.1294650790829927
variance of the performance:  0.024566657057509742
At round 230 training accu: 0.12946727400368974, loss: nan
At round 230 test accu: 0.1309756531011196
At round 230 malicious test accu: nan
At round 230 benign test accu: 0.1309756531011196
variance of the performance:  0.025298280941199902
At round 240 training accu: 0.12918972751465282, loss: nan
At round 240 test accu: 0.13035365203483207
At round 240 malicious test accu: nan
At round 240 benign test accu: 0.13035365203483207
variance of the performance:  0.025280726039521126
At round 250 training accu: 0.12763873242885831, loss: nan
At round 250 test accu: 0.12902079260707305
At round 250 malicious test accu: nan
At round 250 benign test accu: 0.12902079260707305
variance of the performance:  0.025193678213951805
At round 260 training accu: 0.12706731318672349, loss: nan
At round 260 test accu: 0.12795450506486583
At round 260 malicious test accu: nan
At round 260 benign test accu: 0.12795450506486583
variance of the performance:  0.024844736454382635
At round 270 training accu: 0.128683613328762, loss: nan
At round 270 test accu: 0.12919850719744091
At round 270 malicious test accu: nan
At round 270 benign test accu: 0.12919850719744091
variance of the performance:  0.025270058705820376
At round 280 training accu: 0.12785097386165123, loss: nan
At round 280 test accu: 0.12822107695041762
At round 280 malicious test accu: nan
At round 280 benign test accu: 0.12822107695041762
variance of the performance:  0.02619219386426187
At round 290 training accu: 0.12718159703515045, loss: nan
At round 290 test accu: 0.1268882175226586
At round 290 malicious test accu: nan
At round 290 benign test accu: 0.1268882175226586
variance of the performance:  0.025923841988747576
At round 300 training accu: 0.12566325447747792, loss: nan
At round 300 test accu: 0.1255553580948996
At round 300 malicious test accu: nan
At round 300 benign test accu: 0.1255553580948996
variance of the performance:  0.026068165923426544
At round 310 training accu: 0.125402034252502, loss: nan
At round 310 test accu: 0.12475564243824418
At round 310 malicious test accu: nan
At round 310 benign test accu: 0.12475564243824418
variance of the performance:  0.025642240689250608
At round 320 training accu: 0.12512448776346508, loss: nan
At round 320 test accu: 0.12457792784787632
At round 320 malicious test accu: nan
At round 320 benign test accu: 0.12457792784787632
variance of the performance:  0.025433062387791455
At round 330 training accu: 0.12564692821341691, loss: nan
At round 330 test accu: 0.12502221432379598
At round 330 malicious test accu: nan
At round 330 benign test accu: 0.12502221432379598
variance of the performance:  0.026080638070314986
