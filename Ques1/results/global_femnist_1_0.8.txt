Arguments:
	              alpha : 0.0
	         batch_size : 32
	           boosting : 0
	  clients_per_round : 10
	          comm_freq : 0.1
	data_partition_seed : 0
	            dataset : femnist
	       decay_factor : 1.0
	        dynamic_lam : 0
	         eval_every : 10
	            fedmgda : 0
	        fedmgda_eps : 0.0
	     finetune_iters : 40
	         global_reg : -1.0
	  gradient_clipping : 0
	             k_loss : 0
	             k_norm : 0
	               krum : 0
	                lam : 100.0
	       lambda_l2sgd : 0
	      learning_rate : 0.1
	        local_iters : 2
	             median : 0
	              mkrum : 0
	              model : cnn
	       model_params : (62,)
	      num_corrupted : 164
	         num_epochs : 1
	         num_rounds : 1000
	          optimizer : ditto
	                  q : 0.0
	     random_updates : 0
	           sampling : 2
	               seed : 0
Using global-regularized multi-task learning to Train
Parsing Inputs...

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/443.70k flops)
  dense/kernel/Initializer/random_uniform (200.70k/401.41k flops)
    dense/kernel/Initializer/random_uniform/mul (200.70k/200.70k flops)
    dense/kernel/Initializer/random_uniform/sub (1/1 flops)
  conv2d_1/kernel/Initializer/random_uniform (12.80k/25.60k flops)
    conv2d_1/kernel/Initializer/random_uniform/mul (12.80k/12.80k flops)
    conv2d_1/kernel/Initializer/random_uniform/sub (1/1 flops)
  dense_1/kernel/Initializer/random_uniform (7.94k/15.87k flops)
    dense_1/kernel/Initializer/random_uniform/mul (7.94k/7.94k flops)
    dense_1/kernel/Initializer/random_uniform/sub (1/1 flops)
  conv2d/kernel/Initializer/random_uniform (400/801 flops)
    conv2d/kernel/Initializer/random_uniform/mul (400/400 flops)
    conv2d/kernel/Initializer/random_uniform/sub (1/1 flops)
  gradients_1/Sum_grad/Maximum (2/2 flops)
  gradients_1/Sum_1_grad/Maximum (2/2 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)
  sparse_softmax_cross_entropy_loss/div (1/1 flops)
  sparse_softmax_cross_entropy_loss/Greater (1/1 flops)
  sparse_softmax_cross_entropy_loss/Equal (1/1 flops)
  gradients_1/Sum_grad/add (1/1 flops)
  gradients_1/Sum_1_grad/add (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/mul (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/RealDiv_2 (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/RealDiv_1 (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/RealDiv (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/Neg (1/1 flops)
  dropout_1/random_uniform/sub (1/1 flops)
  dropout/random_uniform/sub (1/1 flops)

======================End of Report==========================
206 Clients in Total
---10 workers per communication round---
[  8 190  83 201 173  34  67  54  13  28 181 164 145  15 174 134 139  91
 167  92   9  44  77  48  55  49 129 136  64  43 204  98  41  10  20 196
  17 102  63 162 186 159  86  73 160 146  93  82  52 197   7  88  65 149
  89 132 106   4  87  37  16 182  21  29  35  90  66 121 108  59 153 202
 120 122 168 192  30  95 130  27 110  51  47  24 148  60  62 133  56 151
 138 178 203  38 185   6  68 107 101  11 156  33  36  14 165 118 115  26
 198  57  31  12  80 194  75 112 113  81 193 137 177 169 179 109  22 119
 180  99 152 187 189  97 157  19 111 205 154 123 191  72 142  61 131  85
  23   3 117 172 144  53 135 199   1   0  74  58 184 171  94 188  39 170
 176  40]
At round 10 training accu: 0.01585360883563598, loss: nan
At round 10 test accu: 0.015071348404681738
At round 10 malicious test accu: 0.015163607342378291
At round 10 benign test accu: 0.014693877551020407
variance of the performance:  0.0005754208502612044
