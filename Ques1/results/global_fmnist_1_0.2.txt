Arguments:
	              alpha : 0.0
	         batch_size : 32
	           boosting : 0
	  clients_per_round : 10
	          comm_freq : 0.1
	data_partition_seed : 0
	            dataset : fmnist
	       decay_factor : 1.0
	        dynamic_lam : 0
	         eval_every : 10
	            fedmgda : 0
	        fedmgda_eps : 0.0
	     finetune_iters : 40
	         global_reg : -1.0
	  gradient_clipping : 0
	             k_loss : 0
	             k_norm : 0
	               krum : 0
	                lam : 100.0
	       lambda_l2sgd : 0
	      learning_rate : 0.1
	        local_iters : 2
	             median : 0
	              mkrum : 0
	              model : cnn
	       model_params : (10,)
	      num_corrupted : 100
	         num_epochs : 1
	         num_rounds : 1000
	          optimizer : ditto
	                  q : 0.0
	     random_updates : 0
	           sampling : 2
	               seed : 0
Using global-regularized multi-task learning to Train
Parsing Inputs...

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/6.55m flops)
  dense/kernel/Initializer/random_uniform (3.21m/6.42m flops)
    dense/kernel/Initializer/random_uniform/mul (3.21m/3.21m flops)
    dense/kernel/Initializer/random_uniform/sub (1/1 flops)
  conv2d_1/kernel/Initializer/random_uniform (51.20k/102.40k flops)
    conv2d_1/kernel/Initializer/random_uniform/mul (51.20k/51.20k flops)
    conv2d_1/kernel/Initializer/random_uniform/sub (1/1 flops)
  dense_1/kernel/Initializer/random_uniform (10.24k/20.48k flops)
    dense_1/kernel/Initializer/random_uniform/mul (10.24k/10.24k flops)
    dense_1/kernel/Initializer/random_uniform/sub (1/1 flops)
  conv2d/kernel/Initializer/random_uniform (800/1.60k flops)
    conv2d/kernel/Initializer/random_uniform/mul (800/800 flops)
    conv2d/kernel/Initializer/random_uniform/sub (1/1 flops)
  gradients_1/Sum_1_grad/Maximum (2/2 flops)
  gradients_1/Sum_grad/Maximum (2/2 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)
  sparse_softmax_cross_entropy_loss/div (1/1 flops)
  sparse_softmax_cross_entropy_loss/Greater (1/1 flops)
  sparse_softmax_cross_entropy_loss/Equal (1/1 flops)
  gradients_1/Sum_grad/add (1/1 flops)
  gradients_1/Sum_1_grad/add (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/mul (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/RealDiv_2 (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/RealDiv_1 (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/RealDiv (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/Neg (1/1 flops)

======================End of Report==========================
500 Clients in Total
---10 workers per communication round---
[394 276 255 318 369 457 458  75  62 113 266 365 300 268 250 389 267   9
 378 112 433 436 254 440  57 232 396  19 181  29 398  94 228 265 249  68
 244 163  99 464 468 216 156 449 135  82 284 296 118 331 353 357 351 328
  14  91 311  35 476 167 136 363 119 213 346 205 307 207  43 362 230 337
 417  64 274 122  54  38 321 489 390 409 152 234  98  20 483 187 339 243
 428 208 303 221 299 427  55 329 271 402]
At round 10 training accu: 0.09268420107426817, loss: nan
At round 10 test accu: 0.0891238670694864
At round 10 malicious test accu: 0.09090909090909091
At round 10 benign test accu: 0.08868467670504872
variance of the performance:  0.01127497916687313
At round 20 training accu: 0.1032146413936099, loss: nan
At round 20 test accu: 0.10218588946152479
At round 20 malicious test accu: 0.1035103510351035
At round 20 benign test accu: 0.10186005314437556
variance of the performance:  0.012858201731805694
At round 30 training accu: 0.10293709490457299, loss: nan
At round 30 test accu: 0.10618446774480185
At round 30 malicious test accu: 0.12106210621062106
At round 30 benign test accu: 0.10252435783879539
variance of the performance:  0.012739910120902902
At round 40 training accu: 0.11077370165385055, loss: nan
At round 40 test accu: 0.11569219832948285
At round 40 malicious test accu: 0.13141314131413143
At round 40 benign test accu: 0.11182462356067316
variance of the performance:  0.015073383713080278
At round 50 training accu: 0.11369610292076864, loss: nan
At round 50 test accu: 0.11649191398613826
At round 50 malicious test accu: 0.12421242124212421
At round 50 benign test accu: 0.1145925597874225
variance of the performance:  0.015856458500231995
At round 60 training accu: 0.12032456612953257, loss: nan
At round 60 test accu: 0.12306735382974943
At round 60 malicious test accu: 0.1251125112511251
At round 60 benign test accu: 0.12256421612046059
variance of the performance:  0.016659384378597163
At round 70 training accu: 0.12878157091312795, loss: nan
At round 70 test accu: 0.12884307801670516
At round 70 malicious test accu: 0.1188118811881188
At round 70 benign test accu: 0.1313108945969885
variance of the performance:  0.01935782258091981
At round 80 training accu: 0.12370410279015853, loss: nan
At round 80 test accu: 0.12591078727563532
At round 80 malicious test accu: 0.1224122412241224
At round 80 benign test accu: 0.12677147918511958
variance of the performance:  0.019474686998855743
At round 90 training accu: 0.12306737849177972, loss: nan
At round 90 test accu: 0.12671050293229075
At round 90 malicious test accu: 0.126012601260126
At round 90 benign test accu: 0.12688219663418954
variance of the performance:  0.020068609817544557
At round 100 training accu: 0.12372042905421952, loss: nan
At round 100 test accu: 0.12368935489603697
At round 100 malicious test accu: 0.11251125112511251
At round 100 benign test accu: 0.12643932683790965
variance of the performance:  0.01991801097272108
At round 110 training accu: 0.1273938384679434, loss: nan
At round 110 test accu: 0.12502221432379598
At round 110 malicious test accu: 0.10396039603960396
At round 110 benign test accu: 0.13020372010628875
variance of the performance:  0.021593921185025657
At round 120 training accu: 0.12585916964620986, loss: nan
At round 120 test accu: 0.12315621112493336
At round 120 malicious test accu: 0.10216021602160216
At round 120 benign test accu: 0.1283215234720992
variance of the performance:  0.02201004931422045
At round 130 training accu: 0.12554897062905096, loss: nan
At round 130 test accu: 0.1220010662875422
At round 130 malicious test accu: 0.09720972097209721
At round 130 benign test accu: 0.12810008857395927
variance of the performance:  0.02147185743109995
At round 140 training accu: 0.1250755089712821, loss: nan
At round 140 test accu: 0.12182335169717434
At round 140 malicious test accu: 0.09450945094509451
At round 140 benign test accu: 0.12854295837023916
variance of the performance:  0.022120448999177818
At round 150 training accu: 0.12754077484449233, loss: nan
At round 150 test accu: 0.12342278301048516
At round 150 malicious test accu: 0.09450945094509451
At round 150 benign test accu: 0.13053587245349868
variance of the performance:  0.02298921018070593
