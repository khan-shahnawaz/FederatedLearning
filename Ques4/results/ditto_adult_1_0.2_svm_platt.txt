Arguments:
	              alpha : 0.0
	         batch_size : 32
	           boosting : 0
	  clients_per_round : 10
	          comm_freq : 0.1
	data_partition_seed : 0
	            dataset : adult
	       decay_factor : 1.0
	        dynamic_lam : 0
	         eval_every : 10
	            fedmgda : 0
	        fedmgda_eps : 0.0
	     finetune_iters : 40
	         global_reg : -1.0
	  gradient_clipping : 0
	             k_loss : 0
	             k_norm : 0
	               krum : 0
	                lam : 1.0
	       lambda_l2sgd : 0
	      learning_rate : 0.1
	        local_iters : 2
	             median : 0
	              mkrum : 0
	              model : svm_platt
	       model_params : (2,)
	      num_corrupted : 8
	         num_epochs : 1
	         num_rounds : 1001
	          optimizer : ditto_mce
	                  q : 0.0
	     random_updates : 0
	           sampling : 2
	               seed : 0
['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '5', '6', '7', '8', '9'] clients printed here
Using global-regularized multi-task learning to Train
Parsing Inputs...

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/314 flops)
  Square (62/62 flops)
  gradients/AddN (62/62 flops)
  gradients/Square_grad/Mul (62/62 flops)
  gradients/Square_grad/Mul_1 (62/62 flops)
  Sum (61/61 flops)
  add_1 (1/1 flops)
  gradients/Mean_grad/Maximum (1/1 flops)
  gradients/mul_grad/Mul (1/1 flops)
  gradients/mul_grad/Mul_1 (1/1 flops)
  mul (1/1 flops)

======================End of Report==========================
40 Clients in Total
---10 workers per communication round---
[22 25 24 12 29 17 31  9]
At round 10, Maximum calibaration Error: 0.08020464067589345
At round 10 training accu: 0.7631046715957693, loss: 0.4679464932073748
At round 10 test accu: 0.7593625498007968
At round 10 malicious test accu: 0.125
At round 10 benign test accu: 0.7637737362931265
variance of the performance:  0.12594773036715085
At round 20, Maximum calibaration Error: 0.05730145783561266
At round 20 training accu: 0.7751400815622824, loss: 0.4490434276646249
At round 20 test accu: 0.7710491367861886
At round 20 malicious test accu: 0.125
At round 20 benign test accu: 0.7755415886600695
variance of the performance:  0.06736111339898546
At round 30, Maximum calibaration Error: 0.062103663509658014
At round 30 training accu: 0.7698683730645536, loss: 0.45174197860466997
At round 30 test accu: 0.7656706507304117
At round 30 malicious test accu: 0.1346153846153846
At round 30 benign test accu: 0.7700588392618347
variance of the performance:  0.05161766242123635
At round 40, Maximum calibaration Error: 0.06557803715357191
At round 40 training accu: 0.7958290507609164, loss: 0.43512370896088043
At round 40 test accu: 0.7913678618857902
At round 40 malicious test accu: 0.14423076923076922
At round 40 benign test accu: 0.795867879112062
variance of the performance:  0.0357071910584764
At round 50, Maximum calibaration Error: 0.06158672098723535
At round 50 training accu: 0.8006365836676502, loss: 0.4260751527056104
At round 50 test accu: 0.7956175298804781
At round 50 malicious test accu: 0.14423076923076922
At round 50 benign test accu: 0.8001470981545867
variance of the performance:  0.03774022813113934
At round 60, Maximum calibaration Error: 0.07190643808019037
At round 60 training accu: 0.7888001060972779, loss: 0.4333464436136456
At round 60 test accu: 0.7833997343957503
At round 60 malicious test accu: 0.14423076923076922
At round 60 benign test accu: 0.7878443434073281
variance of the performance:  0.03790850022678775
At round 70, Maximum calibaration Error: 0.08855467645540388
At round 70 training accu: 0.8137661218129373, loss: 0.4258748819399832
At round 70 test accu: 0.8084329349269588
At round 70 malicious test accu: 0.14423076923076922
At round 70 benign test accu: 0.8130516180797005
variance of the performance:  0.037366584818504904
At round 80, Maximum calibaration Error: 0.07808292947041273
At round 80 training accu: 0.8016312456483539, loss: 0.4271704470380271
At round 80 test accu: 0.7956839309428951
At round 80 malicious test accu: 0.15384615384615385
At round 80 benign test accu: 0.8001470981545867
variance of the performance:  0.037582276465610624
At round 90, Maximum calibaration Error: 0.09215015051443923
At round 90 training accu: 0.8106495142733994, loss: 0.42207246153219213
At round 90 test accu: 0.8041832669322709
At round 90 malicious test accu: 0.15384615384615385
At round 90 benign test accu: 0.8087055362396363
variance of the performance:  0.03829771011133358
At round 100, Maximum calibaration Error: 0.12850228881308856
At round 100 training accu: 0.7906236530619011, loss: 0.42901762715469466
At round 100 test accu: 0.7842629482071714
At round 100 malicious test accu: 0.1346153846153846
At round 100 benign test accu: 0.7887804225728805
variance of the performance:  0.038058496881340644
At round 110, Maximum calibaration Error: 0.09775654810187849
At round 110 training accu: 0.8185073439209575, loss: 0.4316071936962147
At round 110 test accu: 0.8110889774236387
At round 110 malicious test accu: 0.125
At round 110 benign test accu: 0.8158598555763573
variance of the performance:  0.02030511122631279
At round 120, Maximum calibaration Error: 0.07063862857682646
At round 120 training accu: 0.8036537250091177, loss: 0.4225189909162327
At round 120 test accu: 0.797808764940239
At round 120 malicious test accu: 0.14423076923076922
At round 120 benign test accu: 0.8023535704733886
variance of the performance:  0.020184255321101158
At round 130, Maximum calibaration Error: 0.07131209999414143
At round 130 training accu: 0.8001724080766552, loss: 0.42453196688049794
At round 130 test accu: 0.7939575033200531
At round 130 malicious test accu: 0.125
At round 130 benign test accu: 0.7986092538111794
variance of the performance:  0.02089717826593106
At round 140, Maximum calibaration Error: 0.11076571349999886
At round 140 training accu: 0.7988130367030271, loss: 0.42810825725652313
At round 140 test accu: 0.7931606905710491
At round 140 malicious test accu: 0.125
At round 140 benign test accu: 0.7978069002407061
variance of the performance:  0.020142915846152895
At round 150, Maximum calibaration Error: 0.10526527385758383
At round 150 training accu: 0.8186068101190279, loss: 0.4206440453376068
At round 150 test accu: 0.8099601593625498
At round 150 malicious test accu: 0.1346153846153846
At round 150 benign test accu: 0.8146563252206472
variance of the performance:  0.02088074886897634
