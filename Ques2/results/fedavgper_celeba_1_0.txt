Arguments:
	              alpha : 0.0
	         batch_size : 32
	           boosting : 0
	  clients_per_round : 10
	          comm_freq : 0.1
	data_partition_seed : 0
	            dataset : celeba
	       decay_factor : 1.0
	        dynamic_lam : 0
	         eval_every : 10
	            fedmgda : 0
	        fedmgda_eps : 0.0
	     finetune_iters : 40
	         global_reg : -1.0
	  gradient_clipping : 0
	             k_loss : 0
	             k_norm : 0
	               krum : 0
	                lam : 0.0
	       lambda_l2sgd : 0
	      learning_rate : 0.1
	        local_iters : 2
	             median : 0
	              mkrum : 0
	              model : cnn
	       model_params : (2,)
	      num_corrupted : 0
	         num_epochs : 1
	         num_rounds : 1000
	          optimizer : fedavgper
	                  q : 0.0
	     random_updates : 0
	           sampling : 2
	               seed : 0
Using fedavgper to Train
Parsing Inputs...

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/62.43k flops)
  conv2d_3/kernel/Initializer/random_uniform (9.22k/18.43k flops)
    conv2d_3/kernel/Initializer/random_uniform/mul (9.22k/9.22k flops)
    conv2d_3/kernel/Initializer/random_uniform/sub (1/1 flops)
  conv2d_2/kernel/Initializer/random_uniform (9.22k/18.43k flops)
    conv2d_2/kernel/Initializer/random_uniform/mul (9.22k/9.22k flops)
    conv2d_2/kernel/Initializer/random_uniform/sub (1/1 flops)
  conv2d_1/kernel/Initializer/random_uniform (9.22k/18.43k flops)
    conv2d_1/kernel/Initializer/random_uniform/mul (9.22k/9.22k flops)
    conv2d_1/kernel/Initializer/random_uniform/sub (1/1 flops)
  dense/kernel/Initializer/random_uniform (2.30k/4.61k flops)
    dense/kernel/Initializer/random_uniform/mul (2.30k/2.30k flops)
    dense/kernel/Initializer/random_uniform/sub (1/1 flops)
  conv2d/kernel/Initializer/random_uniform (864/1.73k flops)
    conv2d/kernel/Initializer/random_uniform/mul (864/864 flops)
    conv2d/kernel/Initializer/random_uniform/sub (1/1 flops)
  batch_normalization/AssignMovingAvg_1 (32/97 flops)
    batch_normalization/AssignMovingAvg_1/mul (32/32 flops)
    batch_normalization/AssignMovingAvg_1/sub_1 (32/32 flops)
    batch_normalization/AssignMovingAvg_1/sub (1/1 flops)
  batch_normalization/AssignMovingAvg (32/97 flops)
    batch_normalization/AssignMovingAvg/mul (32/32 flops)
    batch_normalization/AssignMovingAvg/sub_1 (32/32 flops)
    batch_normalization/AssignMovingAvg/sub (1/1 flops)
  batch_normalization_3/AssignMovingAvg_1 (32/97 flops)
    batch_normalization_3/AssignMovingAvg_1/mul (32/32 flops)
    batch_normalization_3/AssignMovingAvg_1/sub_1 (32/32 flops)
    batch_normalization_3/AssignMovingAvg_1/sub (1/1 flops)
  batch_normalization_3/AssignMovingAvg (32/97 flops)
    batch_normalization_3/AssignMovingAvg/mul (32/32 flops)
    batch_normalization_3/AssignMovingAvg/sub_1 (32/32 flops)
    batch_normalization_3/AssignMovingAvg/sub (1/1 flops)
  batch_normalization_2/AssignMovingAvg_1 (32/97 flops)
    batch_normalization_2/AssignMovingAvg_1/mul (32/32 flops)
    batch_normalization_2/AssignMovingAvg_1/sub_1 (32/32 flops)
    batch_normalization_2/AssignMovingAvg_1/sub (1/1 flops)
  batch_normalization_2/AssignMovingAvg (32/97 flops)
    batch_normalization_2/AssignMovingAvg/mul (32/32 flops)
    batch_normalization_2/AssignMovingAvg/sub_1 (32/32 flops)
    batch_normalization_2/AssignMovingAvg/sub (1/1 flops)
  batch_normalization_1/AssignMovingAvg_1 (32/97 flops)
    batch_normalization_1/AssignMovingAvg_1/mul (32/32 flops)
    batch_normalization_1/AssignMovingAvg_1/sub_1 (32/32 flops)
    batch_normalization_1/AssignMovingAvg_1/sub (1/1 flops)
  batch_normalization_1/AssignMovingAvg (32/97 flops)
    batch_normalization_1/AssignMovingAvg/mul (32/32 flops)
    batch_normalization_1/AssignMovingAvg/sub_1 (32/32 flops)
    batch_normalization_1/AssignMovingAvg/sub (1/1 flops)
  gradients_1/Sum_1_grad/Maximum (2/2 flops)
  gradients_1/Sum_grad/Maximum (2/2 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)
  sparse_softmax_cross_entropy_loss/div (1/1 flops)
  sparse_softmax_cross_entropy_loss/Greater (1/1 flops)
  sparse_softmax_cross_entropy_loss/Equal (1/1 flops)
  gradients_1/Sum_grad/add (1/1 flops)
  gradients_1/Sum_1_grad/add (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/mul (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/RealDiv_2 (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/RealDiv_1 (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/RealDiv (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/Neg (1/1 flops)

======================End of Report==========================
515 Clients in Total
Training with 10 workers ---
[]
At round 0 training accu: 0.5442604856512141, loss: 1.6829907135038777
At round 0 test loss: 1.6998989714446224
At round 0 test accu: 0.5398082534389329
At round 0 malicious test accu: nan
At round 0 benign test accu: 0.5398082534389329
At round 0 variance: 0.10805247501960537
At round 10 training accu: 0.549448123620309, loss: 0.9463123704835148
At round 10 test loss: 1.0132180317862114
At round 10 test accu: 0.5489787411421425
At round 10 malicious test accu: nan
At round 10 benign test accu: 0.5489787411421425
At round 10 variance: 0.10594650629284297
At round 20 training accu: 0.6275938189845475, loss: 0.6527060801060521
At round 20 test loss: 0.6766365718386878
At round 20 test accu: 0.6073363901625677
At round 20 malicious test accu: nan
At round 20 benign test accu: 0.6073363901625677
At round 20 variance: 0.07832741165087016
At round 30 training accu: 0.6421633554083885, loss: 0.6394148513937892
At round 30 test loss: 0.667226928945828
At round 30 test accu: 0.6115047936640267
At round 30 malicious test accu: nan
At round 30 benign test accu: 0.6115047936640267
At round 30 variance: 0.07636898376405264
At round 40 training accu: 0.5594922737306843, loss: 0.7768469898577031
At round 40 test loss: 0.8221594978460872
At round 40 test accu: 0.5543976656940391
At round 40 malicious test accu: nan
At round 40 benign test accu: 0.5543976656940391
At round 40 variance: 0.10492266759344576
At round 50 training accu: 0.6517660044150111, loss: 0.6315444026824918
At round 50 test loss: 0.6675953871759587
At round 50 test accu: 0.6152563568153397
At round 50 malicious test accu: nan
At round 50 benign test accu: 0.6152563568153397
At round 50 variance: 0.07552767951055121
At round 60 training accu: 0.6747240618101545, loss: 0.6057053260192703
At round 60 test loss: 0.6404463258098692
At round 60 test accu: 0.6490204251771572
At round 60 malicious test accu: nan
At round 60 benign test accu: 0.6490204251771572
At round 60 variance: 0.07346376637813827
At round 70 training accu: 0.6997792494481236, loss: 0.5823251255991443
At round 70 test loss: 0.6069471673289156
At round 70 test accu: 0.6823676531888286
At round 70 malicious test accu: nan
At round 70 benign test accu: 0.6823676531888286
At round 70 variance: 0.07264632839656741
At round 80 training accu: 0.7144591611479029, loss: 0.5600819716225923
At round 80 test loss: 0.5928404533031361
At round 80 test accu: 0.6819508128386828
At round 80 malicious test accu: nan
At round 80 benign test accu: 0.6819508128386828
At round 80 variance: 0.07346150927773001
At round 90 training accu: 0.7052980132450332, loss: 0.5822644386613212
At round 90 test loss: 0.6268920788223878
At round 90 test accu: 0.6861192163401417
At round 90 malicious test accu: nan
At round 90 benign test accu: 0.6861192163401417
At round 90 variance: 0.08118006595606479
At round 100 training accu: 0.7942604856512141, loss: 0.44491253097194183
At round 100 test loss: 0.4864101676754079
At round 100 test accu: 0.77157148812005
At round 100 malicious test accu: nan
At round 100 benign test accu: 0.77157148812005
At round 100 variance: 0.0682154393790067
At round 110 training accu: 0.8283664459161147, loss: 0.38616548555699604
At round 110 test loss: 0.4260690034122182
At round 110 test accu: 0.810754481033764
At round 110 malicious test accu: nan
At round 110 benign test accu: 0.810754481033764
At round 110 variance: 0.06373689476418964
At round 120 training accu: 0.8379690949227373, loss: 0.3656843472864257
At round 120 test loss: 0.4029480939410338
At round 120 test accu: 0.8195081283868278
At round 120 malicious test accu: nan
At round 120 benign test accu: 0.8195081283868278
At round 120 variance: 0.05851292564029729
At round 130 training accu: 0.8538631346578367, loss: 0.3343330163993846
At round 130 test loss: 0.36204991701088934
At round 130 test accu: 0.842434347644852
At round 130 malicious test accu: nan
At round 130 benign test accu: 0.842434347644852
At round 130 variance: 0.05093254796498674
At round 140 training accu: 0.8679911699779249, loss: 0.31390880822424033
At round 140 test loss: 0.3334964064621297
At round 140 test accu: 0.8570237598999583
At round 140 malicious test accu: nan
At round 140 benign test accu: 0.8570237598999583
At round 140 variance: 0.0455370092863865
At round 150 training accu: 0.870971302428256, loss: 0.3004382678082472
At round 150 test loss: 0.31863182104238436
At round 150 test accu: 0.8553563984993747
At round 150 malicious test accu: nan
At round 150 benign test accu: 0.8553563984993747
At round 150 variance: 0.044745946891025866
At round 160 training accu: 0.8874172185430463, loss: 0.2773578587929753
At round 160 test loss: 0.3000883477664628
At round 160 test accu: 0.873697373905794
At round 160 malicious test accu: nan
At round 160 benign test accu: 0.873697373905794
At round 160 variance: 0.04666626910920839
At round 170 training accu: 0.8845474613686535, loss: 0.2762614462903452
At round 170 test loss: 0.2994392125291248
At round 170 test accu: 0.870362651104627
At round 170 malicious test accu: nan
At round 170 benign test accu: 0.870362651104627
At round 170 variance: 0.03989274498249357
At round 180 training accu: 0.895916114790287, loss: 0.25405901448133505
At round 180 test loss: 0.2746947460901777
At round 180 test accu: 0.8841183826594414
At round 180 malicious test accu: nan
At round 180 benign test accu: 0.8841183826594414
At round 180 variance: 0.041335664815459604
At round 190 training accu: 0.8874172185430463, loss: 0.2649038117462728
At round 190 test loss: 0.2973908928276867
At round 190 test accu: 0.8732805335556482
At round 190 malicious test accu: nan
At round 190 benign test accu: 0.8732805335556482
At round 190 variance: 0.04963682271224385
At round 200 training accu: 0.8965783664459162, loss: 0.2506493395545456
At round 200 test loss: 0.2841046262193114
At round 200 test accu: 0.882034180908712
At round 200 malicious test accu: nan
At round 200 benign test accu: 0.882034180908712
At round 200 variance: 0.043619021115217066
At round 210 training accu: 0.9018763796909492, loss: 0.24492219147988376
At round 210 test loss: 0.2803546857984311
At round 210 test accu: 0.8832847019591497
At round 210 malicious test accu: nan
At round 210 benign test accu: 0.8832847019591497
At round 210 variance: 0.04477748644938228
At round 220 training accu: 0.9045253863134658, loss: 0.23625171209324952
At round 220 test loss: 0.2750221482137086
At round 220 test accu: 0.8866194247603167
At round 220 malicious test accu: nan
At round 220 benign test accu: 0.8866194247603167
At round 220 variance: 0.04229576607719156
At round 230 training accu: 0.9066225165562913, loss: 0.22803839742439638
At round 230 test loss: 0.26961500230705626
At round 230 test accu: 0.8899541475614839
At round 230 malicious test accu: nan
At round 230 benign test accu: 0.8899541475614839
At round 230 variance: 0.03517339169305828
At round 240 training accu: 0.9149006622516557, loss: 0.2091621025852966
At round 240 test loss: 0.2546820883295431
At round 240 test accu: 0.8962067528136723
At round 240 malicious test accu: nan
At round 240 benign test accu: 0.8962067528136723
At round 240 variance: 0.039356871727231124
At round 250 training accu: 0.9149006622516557, loss: 0.21434985411025878
At round 250 test loss: 0.2570532752888577
At round 250 test accu: 0.8970404335139641
At round 250 malicious test accu: nan
At round 250 benign test accu: 0.8970404335139641
At round 250 variance: 0.038115577647780764
At round 260 training accu: 0.9177704194260485, loss: 0.2097340966056183
At round 260 test loss: 0.2515479494313115
At round 260 test accu: 0.8982909545644019
At round 260 malicious test accu: nan
At round 260 benign test accu: 0.8982909545644019
At round 260 variance: 0.037511268937147235
At round 270 training accu: 0.9169977924944812, loss: 0.20853542572863412
At round 270 test loss: 0.25241676321254913
At round 270 test accu: 0.8970404335139641
At round 270 malicious test accu: nan
At round 270 benign test accu: 0.8970404335139641
At round 270 variance: 0.03390501668352011
At round 280 training accu: 0.915673289183223, loss: 0.21289476429705384
At round 280 test loss: 0.2583945304974075
At round 280 test accu: 0.8966235931638182
At round 280 malicious test accu: nan
At round 280 benign test accu: 0.8966235931638182
At round 280 variance: 0.028989064818751208
At round 290 training accu: 0.92439293598234, loss: 0.1969222532626606
At round 290 test loss: 0.24469173328946014
At round 290 test accu: 0.9028761984160066
At round 290 malicious test accu: nan
At round 290 benign test accu: 0.9028761984160066
At round 290 variance: 0.034006487881230354
At round 300 training accu: 0.9233995584988962, loss: 0.19348838186213
At round 300 test loss: 0.24025412088159837
At round 300 test accu: 0.9066277615673197
At round 300 malicious test accu: nan
At round 300 benign test accu: 0.9066277615673197
At round 300 variance: 0.027912949450996663
At round 310 training accu: 0.927924944812362, loss: 0.18277500496149165
At round 310 test loss: 0.2311329860883699
At round 310 test accu: 0.9116298457690705
At round 310 malicious test accu: nan
At round 310 benign test accu: 0.9116298457690705
At round 310 variance: 0.029660043487656162
At round 320 training accu: 0.9271523178807947, loss: 0.18368036685667974
At round 320 test loss: 0.24109468914763468
At round 320 test accu: 0.9037098791162984
At round 320 malicious test accu: nan
At round 320 benign test accu: 0.9037098791162984
At round 320 variance: 0.02900851521052698
At round 330 training accu: 0.9278145695364238, loss: 0.1811871979925288
At round 330 test loss: 0.23939400931752838
At round 330 test accu: 0.9049604001667362
At round 330 malicious test accu: nan
At round 330 benign test accu: 0.9049604001667362
At round 330 variance: 0.033230267630978624
At round 340 training accu: 0.9291390728476822, loss: 0.1774574840705208
At round 340 test loss: 0.2365110022265386
At round 340 test accu: 0.9082951229679033
At round 340 malicious test accu: nan
At round 340 benign test accu: 0.9082951229679033
At round 340 variance: 0.03395905884897712
At round 350 training accu: 0.9304635761589404, loss: 0.1741140420758962
At round 350 test loss: 0.23617889178699938
At round 350 test accu: 0.9037098791162984
At round 350 malicious test accu: nan
At round 350 benign test accu: 0.9037098791162984
At round 350 variance: 0.027229402836636593
At round 360 training accu: 0.9352097130242826, loss: 0.1668413157050321
At round 360 test loss: 0.23145860851543132
At round 360 test accu: 0.9070446019174656
At round 360 malicious test accu: nan
At round 360 benign test accu: 0.9070446019174656
At round 360 variance: 0.02973500230519204
At round 370 training accu: 0.9336644591611479, loss: 0.16784616738655986
At round 370 test loss: 0.2299554558038452
At round 370 test accu: 0.9049604001667362
At round 370 malicious test accu: nan
At round 370 benign test accu: 0.9049604001667362
At round 370 variance: 0.030564850064196982
At round 380 training accu: 0.932560706401766, loss: 0.16688055244269812
At round 380 test loss: 0.23677005488972191
At round 380 test accu: 0.9074614422676115
At round 380 malicious test accu: nan
At round 380 benign test accu: 0.9074614422676115
At round 380 variance: 0.030002130993472577
At round 390 training accu: 0.93719646799117, loss: 0.16123227732340603
At round 390 test loss: 0.22915595664005342
At round 390 test accu: 0.9078782826177574
At round 390 malicious test accu: nan
At round 390 benign test accu: 0.9078782826177574
At round 390 variance: 0.02901913811680452
At round 400 training accu: 0.9306843267108168, loss: 0.17689974087210286
At round 400 test loss: 0.2400288944270821
At round 400 test accu: 0.9049604001667362
At round 400 malicious test accu: nan
At round 400 benign test accu: 0.9049604001667362
At round 400 variance: 0.026942075664588897
At round 410 training accu: 0.9367549668874172, loss: 0.15921902130151688
At round 410 test loss: 0.23576022321949303
At round 410 test accu: 0.9037098791162984
At round 410 malicious test accu: nan
At round 410 benign test accu: 0.9037098791162984
At round 410 variance: 0.031955848038150124
At round 420 training accu: 0.9427152317880795, loss: 0.1511066560427184
At round 420 test loss: 0.22995663964168342
At round 420 test accu: 0.9087119633180492
At round 420 malicious test accu: nan
At round 420 benign test accu: 0.9087119633180492
At round 420 variance: 0.02616741144567922
At round 430 training accu: 0.9434878587196468, loss: 0.14711879545795883
At round 430 test loss: 0.23133222585943364
At round 430 test accu: 0.9107961650687787
At round 430 malicious test accu: nan
At round 430 benign test accu: 0.9107961650687787
At round 430 variance: 0.028591528734534
