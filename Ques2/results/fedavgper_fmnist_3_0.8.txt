Arguments:
	              alpha : 0.0
	         batch_size : 32
	           boosting : 1
	  clients_per_round : 10
	          comm_freq : 0.1
	data_partition_seed : 0
	            dataset : fmnist
	       decay_factor : 1.0
	        dynamic_lam : 0
	         eval_every : 10
	            fedmgda : 0
	        fedmgda_eps : 0.0
	     finetune_iters : 40
	         global_reg : -1.0
	  gradient_clipping : 0
	             k_loss : 0
	             k_norm : 0
	               krum : 0
	                lam : 0.0
	       lambda_l2sgd : 0
	      learning_rate : 0.1
	        local_iters : 2
	             median : 0
	              mkrum : 0
	              model : cnn
	       model_params : (10,)
	      num_corrupted : 400
	         num_epochs : 1
	         num_rounds : 1000
	          optimizer : fedavgper
	                  q : 0.0
	     random_updates : 0
	           sampling : 2
	               seed : 0
Using fedavgper to Train
Parsing Inputs...

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/6.55m flops)
  dense/kernel/Initializer/random_uniform (3.21m/6.42m flops)
    dense/kernel/Initializer/random_uniform/mul (3.21m/3.21m flops)
    dense/kernel/Initializer/random_uniform/sub (1/1 flops)
  conv2d_1/kernel/Initializer/random_uniform (51.20k/102.40k flops)
    conv2d_1/kernel/Initializer/random_uniform/mul (51.20k/51.20k flops)
    conv2d_1/kernel/Initializer/random_uniform/sub (1/1 flops)
  dense_1/kernel/Initializer/random_uniform (10.24k/20.48k flops)
    dense_1/kernel/Initializer/random_uniform/mul (10.24k/10.24k flops)
    dense_1/kernel/Initializer/random_uniform/sub (1/1 flops)
  conv2d/kernel/Initializer/random_uniform (800/1.60k flops)
    conv2d/kernel/Initializer/random_uniform/mul (800/800 flops)
    conv2d/kernel/Initializer/random_uniform/sub (1/1 flops)
  gradients_1/Sum_1_grad/Maximum (2/2 flops)
  gradients_1/Sum_grad/Maximum (2/2 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)
  sparse_softmax_cross_entropy_loss/div (1/1 flops)
  sparse_softmax_cross_entropy_loss/Greater (1/1 flops)
  sparse_softmax_cross_entropy_loss/Equal (1/1 flops)
  gradients_1/Sum_grad/add (1/1 flops)
  gradients_1/Sum_1_grad/add (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/mul (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/RealDiv_2 (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/RealDiv_1 (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/RealDiv (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/div_grad/Neg (1/1 flops)

======================End of Report==========================
500 Clients in Total
Training with 10 workers ---
[394 276 255 318 369 457 458  75  62 113 266 365 300 268 250 389 267   9
 378 112 433 436 254 440  57 232 396  19 181  29 398  94 228 265 249  68
 244 163  99 464 468 216 156 449 135  82 284 296 118 331 353 357 351 328
  14  91 311  35 476 167 136 363 119 213 346 205 307 207  43 362 230 337
 417  64 274 122  54  38 321 489 390 409 152 234  98  20 483 187 339 243
 428 208 303 221 299 427  55 329 271 402 320 170 165 178  17 404 106 438
   5 200 214 183 235  73 292 416 301 211 453 494 488 345 264  63 354 322
 225 157 277 309  23  22 335  16  44 371 499 233 151 355 180 154 162 442
 444 217  69 141 475 189  87 490  95 204 340  27 281 297 374 308 148  21
 386 176 344 100 446  65 231 137 238 131  67 290 429  49 443 251  92 146
  34 498 134 253 376 120 388 242 349 432 392 456   8  47 272 413 226  77
 410   7 358  15 342 405 202 343 160   3  60  31 247  74 177 169 481 192
 372 223  33  13 139 222  28 287 144  51 285  59 198 495 260 257 179 470
 425  90 164 484 347 282  78 304 469  56 132 129 193 195 280 452 191 182
 172 155 348  81 479 270 418 194  52 323 364  30 496 375 384 145 450 324
 367 305 278  12 448 431 121 313 497  41 377 333 123  24 338 269  37 229
   6  88 101 210 401 206 393 124 115 294 197 199 220  48 408  11 491  10
 312 461 352 263 317 420 107 316 224 138 466  86 117 273  50 382 286 435
  32 149 447 102  39 319 239 310 412 168 245   4  83 293  93 291 314  66
 341  71 385 315   0  89  70 246 395 174 455 133 158 262 441 110  80 283
 159 430 248 424 188 190 474 147 184 185 366 415 108 173 472 252 486 196
  26 381 383 289  36 142 153   2 462 336 485 203 368 241 209 130 237 298
 105  46 109 473]
At round 0 training accu: 0.09573721245367423, loss: 2.321690418506509
At round 0 test loss: 2.3316580274716476
At round 0 test accu: 0.07410698418340146
At round 0 malicious test accu: 0.07396886717418476
At round 0 benign test accu: 0.07460252751732573
At round 0 variance: 0.010901039860784034
At round 10 training accu: 0.09988408352516694, loss: 2.308632258191342
At round 10 test loss: 2.309298186736906
At round 10 test accu: 0.09489959125644215
At round 10 malicious test accu: 0.09351210089762527
At round 10 benign test accu: 0.09987770077456176
At round 10 variance: 0.013330361484136084
At round 20 training accu: 0.10114120585786354, loss: 2.305454451281385
At round 20 test loss: 2.291974611322177
At round 20 test accu: 0.09632130797938511
At round 20 malicious test accu: 0.09578456993523463
At round 20 benign test accu: 0.09824704443538525
At round 20 variance: 0.016160544137083666
At round 30 training accu: 0.11065941780542358, loss: 2.3020232602241344
At round 30 test loss: 2.2968990064671475
At round 30 test accu: 0.14963568508974587
At round 30 malicious test accu: 0.14873309851153277
At round 30 benign test accu: 0.15287403179779863
At round 30 variance: 0.017210882689455195
At round 40 training accu: 0.09960653703613002, loss: 2.3034601339649523
At round 40 test loss: 2.300820258236999
At round 40 test accu: 0.08805757952727919
At round 40 malicious test accu: 0.08737643449607999
At round 40 benign test accu: 0.09050142682429678
At round 40 variance: 0.014408375765520468
At round 50 training accu: 0.10463502636691646, loss: 2.3025486348212527
At round 50 test loss: 2.296404411955034
At round 50 test accu: 0.12075706415496712
At round 50 malicious test accu: 0.1219179638677423
At round 50 benign test accu: 0.11659192825112108
At round 50 variance: 0.013036628580712474
At round 60 training accu: 0.11914907511714094, loss: 2.3012934629075317
At round 60 test loss: 2.2889734087351354
At round 60 test accu: 0.17380486937977607
At round 60 malicious test accu: 0.1700943074650608
At round 60 benign test accu: 0.1871178149205055
At round 60 variance: 0.025941178901361092
At round 70 training accu: 0.10040652397511877, loss: 2.3017609445255176
At round 70 test loss: 2.295251198045437
At round 70 test accu: 0.09969788519637462
At round 70 malicious test accu: 0.10112487217361664
At round 70 benign test accu: 0.09457806767223807
At round 70 variance: 0.014717372757968446
At round 80 training accu: 0.10123916344222951, loss: 2.3004689707650714
At round 80 test loss: 2.2847894708322594
At round 80 test accu: 0.09872045494935135
At round 80 malicious test accu: 0.09726167480968072
At round 80 benign test accu: 0.10395434162250305
At round 80 variance: 0.02372566648590085
At round 90 training accu: 0.11676544056423568, loss: 2.300069770926493
At round 90 test loss: 2.289168973237684
At round 90 test accu: 0.18109116758485871
At round 90 malicious test accu: 0.1864560845358482
At round 90 benign test accu: 0.16184264166326948
At round 90 variance: 0.029857511826569105
At round 100 training accu: 0.11144307848035134, loss: 2.3137420513443425
At round 100 test loss: 2.280241585697271
At round 100 test accu: 0.16491913986138262
At round 100 malicious test accu: 0.17054880127258265
At round 100 benign test accu: 0.144720750101916
At round 100 variance: 0.03280386261237687
At round 110 training accu: 0.1070676397120047, loss: 2.299597896483847
At round 110 test loss: 2.282840923727438
At round 110 test accu: 0.12697707481784254
At round 110 malicious test accu: 0.12998522895125553
At round 110 benign test accu: 0.11618426416632695
At round 110 variance: 0.013444286350203163
At round 120 training accu: 0.12465102610569623, loss: 2.299215087403909
At round 120 test loss: 2.2842764460597724
At round 120 test accu: 0.1889994668562289
At round 120 malicious test accu: 0.19009203499602317
At round 120 benign test accu: 0.18507949449653485
At round 120 variance: 0.02000750819041837
At round 130 training accu: 0.14483028848508595, loss: 2.294811721108958
At round 130 test loss: 2.2578143925613423
At round 130 test accu: 0.2829216278656478
At round 130 malicious test accu: 0.28599022838313826
At round 130 benign test accu: 0.27191194455768447
At round 130 variance: 0.023704577120068335
At round 140 training accu: 0.17057680690927496, loss: 2.290513136379437
At round 140 test loss: 2.2485751747894356
At round 140 test accu: 0.40483383685800606
At round 140 malicious test accu: 0.4039313714350642
At round 140 benign test accu: 0.4080717488789238
At round 140 variance: 0.03834941574486608
At round 150 training accu: 0.13268354802370574, loss: 2.295904952238726
At round 150 test loss: 2.2778484984022147
At round 150 test accu: 0.2188555180380309
At round 150 malicious test accu: 0.21361208953528008
At round 150 benign test accu: 0.23766816143497757
At round 150 variance: 0.020208560901117403
At round 160 training accu: 0.12567958074153893, loss: 2.2923756019858654
At round 160 test loss: 2.2547765609445634
At round 160 test accu: 0.205171494579705
At round 160 malicious test accu: 0.20554482445176683
At round 160 benign test accu: 0.20383204239706482
At round 160 variance: 0.03820148218708454
At round 170 training accu: 0.12292044211523077, loss: 2.2967687580327056
At round 170 test loss: 2.270300785326208
At round 170 test accu: 0.21245779278478763
At round 170 malicious test accu: 0.21770253380297694
At round 170 benign test accu: 0.19364044027721158
At round 170 variance: 0.018104031363239823
At round 180 training accu: 0.12613671613524677, loss: 2.2902809118138934
At round 180 test loss: 2.23828787413418
At round 180 test accu: 0.1992180558023814
At round 180 malicious test accu: 0.19497784342688332
At round 180 benign test accu: 0.2144313086017122
At round 180 variance: 0.018622640985413868
At round 190 training accu: 0.16231571729441152, loss: 2.2889919598515833
At round 190 test loss: 2.246013817046535
At round 190 test accu: 0.34014572596410164
At round 190 malicious test accu: 0.335189183047381
At round 190 benign test accu: 0.3579290664492458
At round 190 variance: 0.04383025931483191
At round 200 training accu: 0.15505052978726877, loss: 2.2869246145285063
At round 200 test loss: 2.227138128991616
At round 200 test accu: 0.33596943309045674
At round 200 malicious test accu: 0.3398477445744802
At round 200 benign test accu: 0.3220546269873624
At round 200 variance: 0.02490605513078879
At round 210 training accu: 0.1684707188454066, loss: 2.2860430362294606
At round 210 test loss: 2.2319794158560993
At round 210 test accu: 0.38990581126710505
At round 210 malicious test accu: 0.3914327917282127
At round 210 benign test accu: 0.38442723196086426
At round 210 variance: 0.03409594671772951
At round 220 training accu: 0.16737685915331996, loss: 2.2898425696281546
At round 220 test loss: 2.2451026975229365
At round 220 test accu: 0.40794384218944374
At round 220 malicious test accu: 0.4171116918531985
At round 220 benign test accu: 0.3750509580105993
At round 220 variance: 0.02727553151525319
